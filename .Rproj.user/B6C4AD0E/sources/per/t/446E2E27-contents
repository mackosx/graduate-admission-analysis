---
title: "Lab 9"
author: "Emily Medema"
date: "March 18, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Basic Neural Nets in R

Let's start with using the $neuralnet$ package to approximate the linear model for the cars data, as shown in lecture.

```{r}
library(neuralnet)
#?neuralnet
data(cars)
plot(cars)
attach(cars)
carlm<-lm(dist~speed, data = cars)
summary(carlm)
#as soon as you run neural nets you are going to lose all of the interpretability that we have with a lm (as seen in summary)
sum(carlm$residuals^2)
nn<-neuralnet(dist~speed, data = cars, hidden=0)
#0 hidden variables 
#hidden is a vector of integers specifying the number of hidden neurons in each layer
#since it is 0 it essentially means the linear model
plot(nn)
sum((predict(nn, data.frame(speed))-dist)^2)
```

You can't model dist~. you would have to type out everything cause it can be finicky
You can get around it but it's tricky

Note that the $neuralnet$ package can only deal wit numeric responses. If we have a binary response, we can basically approximate a classification scheme by pretending we are modelling the probability (but of course, it might suggest values outside the realm of probability aka <0 or >1). Also $neuralnet$ cannot take simplified formulas (see above) instead, one has to explicitly type out each response and predictor by name in the formula. 

Fortunately, there are many neural net packages out there... $nnet$ is another one. There is no built in plotting function for it, so we'll also install $neuralNetTools$ for visualization purposes. Let's look at the body example from class.

```{r}
library(gclus)
data(body)
sbod<-cbind(scale(body[,1:24]), factor(body[,25]))
colnames(sbod)[25]<-"Gender"
library(nnet)
nnbod2<-nnet(factor(Gender)~., data = sbod, size = 4) #4 hidden var
table(body[,25], predict(nnbod2, type = "class"))
library(NeuralNetTools)
plotnet(nnbod2)
```

We are overfitting as there are 0 misclassifications, Let's set up training and testing sets.

```{r}
set.seed(53747958)
bindex<-sample(1:nrow(sbod), 250)
btrain<-sbod[bindex,]
btest<-sbod[-bindex,]
nnbodtr<-nnet(factor(Gender)~., data=btrain, size = 4)
table(btest[,25], predict(nnbodtr, btest[,-25], type = "class"))
```

In lecture, we left it at that. But we can easily set up a loop to investigate if there's a better option for the number of hidden layer variables, according to the training error. Since it fits relatively quickly, let's make a quick loop. Note that $nnet$ has an annoying printout whenever it is fitted. The help file doesn't do much help... but a bit of googling gives us $trace = FALSE$ as the argument

```{r}
for(i in 1:10){
  nnbodtr<-nnet(factor(Gender)~., data = btrain, size=i, trace = FALSE)
  print(paste("Number of hidden variables: ", i))
  print(table(btest[,25], predict(nnbodtr, newdata = btest[,-25], type = "class")))
}
```

From this fit, it appears the best neural net (ignoring changing activation functions, number of hidden layers, or any other tuning parameters) will result in a predicted misclassification of 1.6%. We can compare this to LDA

```{r}
library(MASS)
blda<-lda(factor(Gender)~., data = data.frame(btrain))
table(btest[,25], predict(blda, newdata = data.frame(btest[,-25]))$class)
```

LDA is approximately equal in terms of misclassification/predictive power on this data. BUT we get better inference with LDA. LDA provides us with a structure for inference as well (we can investigate the estimated mean vector of each group, the convariances, decision boundary, etc.). What about Random Forests?

```{r}
library(randomForest)
rfbod<-randomForest(factor(Gender)~., data = data.frame(btrain))
rfbod
```

Once more, since the output provides OOB error rate, they are believable for future predictions. That said, let's look at predicting for the test set for a direct comparision to the LDA and NN models...

```{r}
table(btest[,25], predict(rfbod, newdata = data.frame(btest[,-25])))
```

All the results point to the RF model being less strong on this data versus LDA or NN.

## PCA on Cars Data

let's run PCA on the cars93 data.

```{r}
card<-read.csv("~/car93.csv", stringsAsFactors = FALSE)
pcard<-prcomp(as.matrix(card[,-c(1,2,3)]), scale.=TRUE)
summary(pcard)
```

According to the Kaiser Criterion, we would retain 2 components which explain 78% of the variation in the data. We can view the data projected onto tjese components in a biplot
```{r}
biplot(pcard)
```

Note that, this is equivalent to plotting the scores (which are the projected/rotated data). We can verify this by simply plotting 

```{r}
plot(pcard$x[,1:2])
```

Of course , the biplot labels the data by observation number, easy enough


```{r}
plot(pcard$x[,1:2],, type = "n")
text(pcard$x[,1], pcard$x[,2], labls = 1:nrow(card))
```

Let's take a look at the component loading (eigenvectors) which provide the coefficients of the original variables. We'll round these so that they are a bit more easily interpreted

```{r}
round(pcard$rotation[,1:2],2)
```

We can assume that the PC1 has to do with the size of the vehical due to the positive engine size, wheel size, and low MPG etc. 

AKA. Focusing on the first component we see that the magnitudes of the coefficients are pretty similar across the board. But note that while most are positively valued, there are several negative values. If we had to summarize the positive ones, most of these measurements have something to do with the size of the vehicle (or some positive correlation), while the negative ones are related to fuel efficiency (neg correlated with size). As such we can broadly interpret the first component as $size$ of the car

The second component is a little less clear. Let's do our best to focus on the larger magnitudes.

```{r}
round(pcard$rotation[,2],2)[abs(pcard$rotation[,2])>.2]
```

So we have a high positive loading of price, horsepower, and RPM along with large negative loadings for MPG and size of rear seats and luggage. What would you envision for a car that scores high on this component? Probably sports cars. We could therefore considerr this a measure of "sportiness"... relatively small, expensive cars with strong engines

This makes some sense from a rotation standpoint. If the bulk of the variation (first component) will measure cars based on their size contrasted with fuel efficiency, sports cars would not 'fit in' with the rest of the data on that component, as they are small cars that are not fuel efficient.

Proof of Concept on these components, here are the four cars that scored highest on PC1 (size).

```{r}
card[order(pcard$x[,1], decreasing=TRUE)[1:4], 1:3]
```

All four cars are considered quite large cars for this data set. And here are the four that scored highest on PC2

```{r}
card[order(pcard$x[,2], decreasing=TRUE)[1:4], 1:3]
```

While not necessarily sporty (only the first is sporty, the rest are "luxury" midsize vehicles (probably SUV tbh)), we can consider them a measure of luxury perhaps. Nonetheless, the cars that scored high are expensive and fuel inefficient relative to their size.

Now that we have the data projected onto the principal components, there is nothing stopping us from moving forward with other analyses we have discussed during this course. For example, let's try some clustering on the PCA data

##Clustering on PCA

First let's show that if we do NOT reduce components, then the distances between observations are retained.

```{r}
test1<-hclust(dist(scale(card[,-c(1,2,3)])))
plot(test1)
test2<-hclust(dist(pcard$x))
plot(test2)
```

These dendrograms are identical. But frankly, we don't need to eyeball it. We can have R compare distance matrices. Note that the $scale$ call adds in a bunch of attributes that we won't be held in the $prcomp$ object, so we set $check.attributes=FALSE$ so that we are only comparing pairwise distances contained in the objects

```{r}
all.equal(dist(scale(card[,-c(1,2,3)])), dist(pcard$x), check.attributes = FALSE)
```

On the other hand, if we reduce components (say to the two we used earlier), then there is some loss of information
```{r}
all.equal(dist(scale(card[,-c(1,2,3)])), dist(pcard$x[,1:2]), check.attributes = FALSE)
```

Let's look at clustering applied to the scores on the first two components
```{r}
pcclust<-hclust(dist(pcard$x[,1:2]))
plot(pcclust)
```

This dendrogram suggests two groups. Let's compare them to car types and see what falls out...

```{r}
table(card$Type, cutree(pcclust,2))
```

The classification table shows that the predominant group structure on the first two principle components separates out the smaller cars (Compact, Small, Sporty) and larger cars (Midsize,large). This matches up well with the interpretation on the first principle component (which is where the bulk of the variance lies).

##PCA lecture example

```{r}
library(HSAUR2)
data(heptathlon)
head(heptathlon)
pcahepu<-prcomp(heptathlon[,-8])
#Scree plot
plot(pcahepu, type="lines")
#loadings, rotation matrix(eigenvectors), how the variables are correlated with each principal component
pcahepu$rotation[,1:3]
round(pcahepu$rotation[,1:3],2)
summary(pcahepu)

pcahep<-prcomp(heptathlon[,-8], scale.=TRUE)
plot(pcahep, type = "lines")
pcahep$rotation
round(pcahep$rotation[,1:3],2)
summary(pcahep)
biplot(pcahep)

pcahep$x

print(cbind(-sort(pcahep$x[,1]), rownames(heptathlon), heptathlon$score))
plot(-pcahep$x[,1], heptathlon$score)
```

