---
title: "Project PCA"
author: "Chelsey"
date: "March 30, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project PCA 

Loading in and exploring the data 
```{r}
admissionsData <- read.csv("Admission_Predict_Ver1.1.csv", header = TRUE)
#head(admissionsData)
```


## With Response Variable Chance.of.Admit

The variable we are interested in predicting, Chance.of.Admit, is the 9th variable.  

Run PCA on the data and remove the response variable (chance of admit) and the unique identifier (serial number)
```{r}
set.seed(43849)
pca.admin <- prcomp(as.matrix(admissionsData[,-c(1,9)]), scale = TRUE)
summary(pca.admin)
```

To choose the number of principal components to keep, we can either use the Kaiser criterian, cumulative proportion/percent of variance, or a scree plot.  

Using the Kaiser criterian, we keep all principal components with a standard deviation greater than 1 (since the data is scaled).  Hence the Kaiser criterian is telling us to keep only the first principal component.  

I will now compare this with a scree plot.  
```{r}
plot(pca.admin, type="lines")
```

The above scree plot plots the monotonically decreasing eigenvalues and the location of an 'elbow' or plateau indicates the number of principal components.  The scree plot suggests probably 2 principal components.

The first two principal components that will be retained explain 78% of the variation in the data.  We can now view the data projected onto the components using a biplot.  

```{r}
biplot(pca.admin)
```


```{r}
plot(pca.admin$x[,1:2])
```

We can put data labels on the biplot by observation number 
```{r}
plot(pca.admin$x[,1:2], type = "n")
text(pca.admin$x[,1:2], labels = 1:nrow(admissionsData))
```

It looks like there are two groups in the above principal component plots.  


Take a look at the component loadings (eigenvectors) which provide the coefficients of the original variables, rounded to 2 decimal places.
```{r}
round(pca.admin$rotation[,1:2], 2)
```

These are the coefficients of the original variables.  The magnitudes are pretty similar for the first component, perhaps with the exception of research.  They are also all containing the same sign.  This is a little difficult to interpret, but most likely indicates that the first principal component is equally weighting all predictor variables, with the exception of research.  

In the second component, the highest magnitude is the research aspect, along with the letter of recommendation. Perhaps this component indicates previous experience a student has.  A reference letter most likely comes from someone you have worked with, conducted research with, volunteered with, or TA'd for.  Therefore a good reference letter coupled with research experience could be indicative of research and other activities in both academic and non-academic settings.  


We can now look at the four students who scored highest on PC1: 
```{r}
admissionsData[order(pca.admin$x[,1], decreasing = TRUE)[1:4],1:9]
```

It is noted that the four students who performed highest on PC1 all had a low belief of their chance of admit.  None of them had research, and all had a similar cumulative GPA.  In addition, the universities where all rated low (1 to be exact) and the students had similar GRE and TOEFL scores (well below the average).  These students in general seem to be ones who are not performing scoring very well across all predictors.  

And the four students who scored highest on PC2:
```{r}
admissionsData[order(pca.admin$x[,2], decreasing = TRUE)[1:4], 1:9]
```

Notice that the four students who performed highest on PC2 all have research experience.  In general, these students are scoring better than the students in principal component 1 across the board.





## With Response Variable Research

The variable we are interested in predicting, Chance.of.Admit, is the 8th variable.  

Run PCA on the data and remove the response variable (research) and the unique identifier (serial number)
```{r}
set.seed(43849)
pca.admin2 <- prcomp(as.matrix(admissionsData[,-c(1,8)]), scale = TRUE)
summary(pca.admin2)
```

To choose the number of principal components to keep, we can either use the Kaiser criterian, cumulative proportion/percent of variance, or a scree plot.  

Using the Kaiser criterian, we keep all principal components with a standard deviation greater than 1 (since the data is scaled).  Hence the Kaiser criterian is telling us to keep the first principal component.  

I will now compare this with a scree plot.  
```{r}
plot(pca.admin2, type="lines")
```

The above scree plot plots the monotonically decreasing eigenvalues and the location of an 'elbow' or plateau indicates the number of principal components.  The scree plot suggests probably 2 principal components.

The first two principal components that will be retained explain 84% of the variation in the data.  We can now view the data projected onto the components using a biplot.  

```{r}
biplot(pca.admin2)
```


```{r}
plot(pca.admin2$x[,1:2])
```

We can put data labels on the biplot by observation number 
```{r}
plot(pca.admin2$x[,1:2], type = "n")
text(pca.admin2$x[,1:2], labels = 1:nrow(admissionsData))
```

It looks like there are two groups in the above principal component plots.  


Take a look at the component loadings (eigenvectors) which provide the coefficients of the original variables, rounded to 2 decimal places.
```{r}
round(pca.admin2$rotation[,1:2], 2)
```

These are the coefficients of the original variables.  The magnitudes are extremely similar for the first component.  They are also all containing the same sign.  This is a little difficult to interpret again, but most likely indicates that the first principal component is equally weighting all predictor variables.  

In the second component, the highest magnitude is the lettor of recommendation which has a negative sign.  Other variables with the same sign include the SOP score and the university rating.  Variables of opposite sign with higher magnitude include GRE Score, TOEFL Score, as well as CGPA and Chance of Admit having a lower magnitude.  Students who score high on this principal component, likely scored high on their standardized tests.  


We can now look at the four students who scored highest on PC1: 
```{r}
admissionsData[order(pca.admin2$x[,1], decreasing = TRUE)[1:4],1:9]
```

The top four students in this first principal component are the same as the first four students in the previous PC1 (compared using Serial.No.).  Even when looking at the loadings, this principal component is very similar to the principal component in the previous section.  

And the four students who scored highest on PC2:
```{r}
admissionsData[order(pca.admin2$x[,2], decreasing = TRUE)[1:4], 1:9]
```

As hypothesized above, the first four students in PC2 are scoring higher on their standardized tests (GRE.Score and TOEFL.Score).  These students are performing the at, or above average on these standardized tests.  However, they all have a below average score on SOP, and LOR.  The CGPA of the students scoring high on PC2 hovers fairly close to the mean.  This proves the initial hypothesis that standardized testing is most important for PC2.  


## Trying to Cluster on the First Two Principal Components 

It appeared that in the first PCA analysis, with the predictor chance.of.Admit removed, there were two groups in the remaining principal component plots.  Here we will perform hierarchical clustering to try to find these groups.  
```{r}
set.seed(574847)
clusts <- hclust(dist(pca.admin$x[,1:2]), method="average")
plot(clusts)
plot(pca.admin$x[,1:2], col=cutree(clusts,2))
```

These are not quiet the groups we noticed by eye.  



